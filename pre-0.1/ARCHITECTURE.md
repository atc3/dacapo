# Dacapo Architecture
<details>

<summary>

## Model
</summary>

A model is just a name with an Architecture. To add support for a new `Architecture`, 
you must create an `attr.s` class that subclasses the `ArchitectureABC` like this:

```python
import attr

from .architecture_abc import ArchitectureABC

from typing import List, Optional


@attr.s
class MyArchitecture(ArchitectureABC):
    # standard model attributes
    input_shape: List[int] = attr.ib()
    output_shape: Optional[List[int]] = attr.ib()
    fmaps_out: int = attr.ib()

    predict_input_shape: Optional[List[int]] = attr.ib()
    predict_output_shape: Optional[List[int]] = attr.ib()

    def instantiate(self, fmaps_in: int):
        return MyTorchModule(fmaps_in, *args, **kwargs)
```

`Architecture`s require the `instantiate` method, which takes `fmaps_in`
and returns a `torch` module that can take a volume of size `input_shape`
with `fmaps_in` channels and return a volume of size `output_shape` with
`fmaps_out` channels.
`Architecture`s must have the `input_shape`, `output_shape`, and `fmaps_out`
properties. These will be used for determining batch sizes during training.
`Architecture`s may also provide the `predict_input_shape` and `predict_output_shape`
arguments to make prediction more efficient. Often times you can get significantly
larger chunks through your model during prediction since you don't have
the memory constraints of keeping gradients and a large batch in memory.


Once you have added support for a new `Architecture`, you must:
1) import it into `__init__.py`
2) include it in the `AnyArchitecture` Union type.
3) Add it to the list of exposed configurable types in `dacapo.configurables`. This Allows
the dacapo-dashboard user interface to read the parameters with their types and metadata.

</summary>
</details>

<details>
<summary>

## Tasks </summary>

### How they work:
Tasks are split up into
1) predictors
2) losses
3) post_processors

Tasks also have data augmentations defined on them. These are here temporarily since
we do not yet have a batch_generator config. Data augmentations can improve your training
by mirroring/rotating/stretching/adding noise/randomising intensity and more.

#### Predictor
A `Predictor` is basically a "head" on the "backbone" defined in your `Model` config.
It takes in the features generated by your model and transforms them into a usable output.
For example generating affinities for instance segmentation. For more info read the
`Predictor` README at "dacapo/tasks/predictors/README.md", or see below.

<details>
<summary>Adding a new predictor </summary>
    
To add support for a new `Predictor`, you must create an `attr.s` class that
subclasses the `PredictorABC` like this:

```python
from .predictor_abc import PredictorABC

import attr


@attr.s
class MyPredictor(PredictorABC):
    name: str = attr.ib(default="my_predictor")
    fmaps_out: int = attr.ib(default=3)

    param_a: Optional[float] = attr.ib(default=None)
    param_b: Optional[int] = attr.ib(default=None)

    # attributes that can be read from other configurable classes
    fmaps_in: Optional[int] = attr.ib(default=None) # read from model
    dims: Optional[int] = attr.ib(default=None) # read from dataset

    def head(self, fmaps_in: int):
        conv_layer = MyConvLayer(fmaps_in, self.fmaps_out)
        return torch.nn.Sequential(conv_layer, sigmoid)

    def add_target(self, gt, target, weights=None, mask=None):

        target_node = MyTargetNode(*args, **kwargs)
        weights_node = MyWeightsNode(*args, **kwargs)

        return target_node, weights_node
```

`Predictors` require the `head` method, which recieves the `fmaps_in` from `Model`,
and the `add_target` function that should provide a node that generates the targets
from the ground truth. It can also optionally also provide a node to generate
training weights.
`Predictors` must also have a property `fmaps_out`. This is to let DaCapo initialize
zarr datasets into which we can write. This can either be a configurable `attr.ib`
or a `@property` defined on your `Predictor`.

Once you have added support for a new Predictor, you must:
1) import it into `__init__.py`
2) include it in the `AnyPredictor` Union type.
3) Add it to the list of exposed configurable types in `dacapo.configurables`. This Allows
the dacapo-dashboard user interface to read the parameters with their types and metadata.

</details>


#### Loss
Each `Predictor` needs to have its own `Loss`. Total loss is the product of each loss applied
to each `Predictor` output and the ground truth.

<details>
<summary>Adding a new loss</summary>

To add support for a new Loss, you must create an `attr.s` class that
subclasses the `LossABC` like this:

```python
from .loss_abc import LossABC

import attr

from typing import Optional


@attr.s
class MyLoss(LossABC):
    param_a: float = attr.ib()

    def instantiate(self):
        return MyTorchLoss(param_a)
```

The only required function is `instantiate`, that returns a torch Module. The returned
module will then recieve the predictions, targets, and optional weights with which
to compute the loss.

Once you have added support for a new loss, you must:
1) import it into `__init__.py`
2) include it in the `AnyLoss` Union type.
3) Add it to the list of exposed configurable types in `dacapo.configurables`. This Allows
the dacapo-dashboard user interface to read the parameters with their types and metadata.
</details>

#### PostProcessor
A `PostProcessor` takes the output created by a predictor and transforms it into a more
usable format. For example taking affinities produced by an affinities `Predictor`, and
running watershed+agglomeration to generate whole segmentations. For more info read the
`PostProcessor` README at "dacapo/tasks/post_processors/README.md", or see below.
<details>
<summary>Adding a new postprocessor</summary>

To add support for a new PostProcessor, you must create an `attr.s` class that
subclasses the `PostProcessorABC` like this:

```python
from .post_processor_abc import PostProcessorABC
from .steps import MyStep1, MyStep2

import attr


@attr.s
class MyPostprocessor(PostProcessorABC):
    name: str = attr.ib(default="MyPostprocessor")

    step1_parameter: List[float] = attr.ib(factory=list)
    step2_parameter: List[int] = attr.ib(factory=list)

    def tasks(self, pred_id, container, in_dataset, out_dataset):
        """
        pred_id: should be the unique id of the predictions you are post processing.
            i.e. f"{run.id}_validation_{iteration}" if run during validation

        container: The zarr container where the data to postprocess exists, and
            where the post processed data will be written. Any intermediate
            data will also be written here

        in_dataset: The dataset in which to find data for postprocessing. 
            Usually "volumes/{predictor_name}"

        out_dataset: The dataset in which to store the post_processed data.
            Usually "volumes/{predictor_name}_{post_processor_name}_{i}", for
            i, parameters in enumerate(parameters).
        """
        half_way_dataset = "volumes/half_way"
        tasks, parameters = MyStep1(step1_parameter=self.step1_parameter).tasks(
            pred_id, container, in_dataset, half_way_dataset
        )
        tasks, parameters = MyStep2(step2_parameter=self.step2_parameter).tasks(
            pred_id, container, half_way_dataset, out_dataset,
            upstream_tasks=(tasks, parameters),
        )

        return tasks, parameters

```

The only required function is `tasks`, that returns a list of daisy tasks,
along with a list of parameters that were used to generate the output. These
tasks are all assumed to be cpu only tasks.

Once you have added support for a new PostProcessor, you must:
1) import it into `__init__.py`
2) include it in the `AnyPostProcessor` Union type.
3) Add it to the list of exposed configurable types in `dacapo.configurables`. This Allows
the dacapo-dashboard user interface to read the parameters with their types and metadata.
</details>

<details>
<summary>Adding a new augmentation</summary>

To add support for a new augmentation, you must create an `attr.s` class that
subclasses the `AugmetABC` like this:

```python
from .augment_abc import AugmentABC

import attr


@attr.s
class MyAugment(AugmentABC):
    param_a: int = attr.ib(metadata={"help_text": "an integer parameter"})
    param_b: float = attr.ib(metadata={"help_text": "a float parameter"})
    param_c: str = attr.ib(metadata={"help_text": "a string parameter"})
    param_d: bool = attr.ib(metadata={"help_text": "a bool parameter"})

    def node(self, array):
        return MyGunpowderAugmentNode(self.param_a, self.param_b, self.param_c, self.param_d)
```

The only required function is `node`, that returns a gunpowder node that
applies your desired augmentation. `node` will always recieve an arraykey
for raw which you may use (like IntensityAugment), or simply operate on everything that is
requested (like SimpleAugment or ElasticAugment).

Once you have added support for a new augmentation, you must:
1) import it into `__init__.py`
2) include it in the `AnyAugment` Union type.
3) Add it to the list of exposed configurable types in `dacapo.configurables`. This Allows
the dacapo-dashboard user interface to read the parameters with their types and metadata.
</details>


</details>

<details>
<summary>

## Optimizer</summary>

An optimizer is just a name with an Algorithm. To add support for a new `Algorithm`, 
you must create an `attr.s` class that subclasses the `AlgorithmABC` like this:

```python
from .algorithm_abc import Algorithm

from typing import Tuple
import attr


@attr.s
class MyAlgorithm(Algorithm):
    my_param1: float = attr.ib(default=0.01)

    def instance(self, params):
        return MyTorchOptimizer(
            params,
            my_param1=self.my_param1
        )
```

`Algorithms` require the `instance` method, which takes in a set of parameters
to optimize, and returns a torch optimizer that can iteratively upgrade the parameters
based on their gradients.

Once you have added support for a new `Algorithm`, you must:
1) import it into `__init__.py`
2) include it in the `AnyAlgorithm` Union type.
3) Add it to the list of exposed configurable types in `dacapo.configurables`. This Allows
the dacapo-dashboard user interface to read the parameters with their types and metadata.
</details>